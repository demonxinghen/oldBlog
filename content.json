[{"title":"图解ConcurrentHashMap","date":"2019-03-20T12:55:04.000Z","path":"2019/03/20/图解ConcurrentHashMap/","text":"概述上篇文章介绍了 HashMap 在多线程并发情况下是不安全的，多线程并发推荐使用 ConcurrentHashMap ，那么 ConcurrentHashMap 是什么？它的设计思想是什么，源码是怎么实现的？ ConcurrentHashMap是什么Concurrent翻译过来是并发的意思，字面理解它的作用是处理并发情况的 HashMap，在介绍它之前先回顾下之前的知识。通过前面两篇学习，我们知道多线程并发下 HashMap 是不安全的(如死循环)，更普遍的是多线程并发下，由于堆内存对于各个线程是共享的，而 HashMap 的 put 方法不是原子操作，假设Thread1先 put 值，然后 sleep 2秒(也可以是系统时间片切换失去执行权)，在这2秒内值被Thread2改了，Thread1“醒来”再 get 的时候发现已经不是原来的值了，这就容易出问题。 那么如何避免这种多线程“奥迪变奥拓”的情况呢？常规思路就是给 HashMap 的 put 方法加锁(synchronized)，保证同一个时刻只允许一个线程拥有对 hashmap 有写的操作权限即可。然而假如线程1中操作耗时，占着茅坑半天不出来，其他需要操作该 hashmap 的线程就需要在门口排队半天，严重影响用户体验(HashTable 就是这么干的)。举个生活中的例子，很多银行除了存取钱，还支持存取贵重物品，贵重物品都放在保险箱里，把 HashMap 和 HashTable 比作银行，结构： 把线程比作人，对应的情况如下： HashMap牌银行：我们的服务宗旨是不用排队，同一时间多人都有机会修改保险柜里的东西，你以为你存的是美元？取出来的其实是日元，破产就在一瞬间，刺不刺激。 HashTable牌银行：我们的服务宗旨是要排队，同一时间只有一个人有机会修改保险柜里的东西，其余的人只能看不能动手改，保你存的是美元取得还是美元。什么？你说如果那人在里面睡着了不出来怎么办？不要着急，来，坐下来打会麻将等他出来。 多线程下用 HashMap 不确定性太高，有破产的风险，不能选；用 HashTable 不会破产，但是用户体验不太好，那么怎样才能做到多人存取既不影响他人存值，又不用排队呢？有人提议搞个「银行者联盟」，多开几个像HashTable 这种「带锁」的银行就好了，有多少人办理业务，就开多少个银行，一对一服务，这个区都是大老板，开银行的成本都是小钱，于是「银行者联盟」成立了。 接下来的情况是这样的：比如盖伦和亚索一起去银行存他们的大宝剑，这个「银行者联盟」一顿操作，然后对盖伦说，1号银行现在没人，你可以去那存，不用排队，然后盖伦就去1号银行存他的大宝剑，1号银行把盖伦接进门，马上拉闸，一顿操作，然后把盖伦的大宝剑放在第x行第x个保险箱，等盖伦办妥离开后，再开闸；同样「银行者联盟」对亚索说，2号银行现在没人，你可以去那存，不用排队，然后亚索去2号银行存他的大宝剑，2号银行把亚索接进门，马上拉闸，一顿操作把亚索的大宝剑放在第x行第x号保险箱，等亚索离开后再开闸，此时不管盖伦和亚索在各自银行里面待多久都不会影响到彼此，不用担心自己的大宝剑被人偷换了。这就是ConcurrentHashMap的设计思路，用一个图来理解 从上图可以看出，此时锁的是对应的单个银行，而不是整个「银行者联盟」。分析下这种设计的特点： 多个银行组成的「银行者联盟」 当有人来办理业务时，「银行者联盟」需要确定这个人去哪个银行 当此人去到指定银行办理业务后，该银行上锁，其他人不能同时执行修改操作，直到此人离开后解锁 由这几点基本思想可以引发一些思考，比如： 1.成立「银行者联盟」时初识银行数是多少？怎么设计合理？ 上面这张图没有给出是否需要排队的结论，这是因为需要结合实际情况分析，比如初识化有16个银行，只有两个人来办理业务，那自然不需要排队；如果现在16个银行都有人在办理业务，这时候来了第17个人，那么他还是需要排队的。由于「银行者联盟」事先无法得知会有多少人来办理业务，所以在它创立的时候需要制定一个「标准」，即初始银行数量，人多的情况「银行者联盟」应该多开几家银行，避免别人排队；人少的情况应该少开，避免浪费钱(什么，你说不差钱？那也不行) 2.当有人来办理业务的时候，「银行者联盟」怎么确定此人去哪个银行？ 正常情况下，如果所有银行都是未上锁状态，那么有人来办理业务去哪都不用排队，当其中有些银行已经上锁，那么后续「银行者联盟」给人推荐的时候就不能把客户往上锁的银行引了，否则分分钟给人锤成麻瓜。因此「银行者联盟」需要时刻保持清醒的头脑，对自己的银行空闲情况了如指掌，每次给用户推荐都应该是最好的选择。 3.「银行者联盟」怎么保证同一时间不会有两个人在同一个银行拥有存权限？ 通过对指定银行加锁/解锁的方式实现。 源码分析Java7 源码分析通过 Java7 的源码分析下代码实现，先看下一些重要的成员 12345678910111213141516171819202122232425262728293031323334353637383940//默认的数组大小16(HashMap里的那个数组)static final int DEFAULT_INITIAL_CAPACITY = 16;//扩容因子0.75static final float DEFAULT_LOAD_FACTOR = 0.75f; //ConcurrentHashMap中的数组final Segment&lt;K,V&gt;[] segments//默认并发标准16static final int DEFAULT_CONCURRENCY_LEVEL = 16;//Segment是ReentrantLock子类，因此拥有锁的操作 static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; //HashMap的那一套，分别是数组、键值对数量、阈值、负载因子 transient volatile HashEntry&lt;K,V&gt;[] table; transient int count; transient int threshold; final float loadFactor; Segment(float lf, int threshold, HashEntry&lt;K,V&gt;[] tab) &#123; this.loadFactor = lf; this.threshold = threshold; this.table = tab; &#125; &#125; //换了马甲还是认识你！！！HashEntry对象，存key、value、hash值以及下一个节点 static final class HashEntry&lt;K,V&gt; &#123; final int hash; final K key; volatile V value; volatile HashEntry&lt;K,V&gt; next; &#125;//segment中HashEntry[]数组最小长度static final int MIN_SEGMENT_TABLE_CAPACITY = 2;//用于定位在segments数组中的位置，下面介绍final int segmentMask;final int segmentShift; 上面这些一下出来有点接受不了没关系，下面都会介绍到。 接下来从最简单的初识化开始分析 1ConcurrentHashMap concurrentHashMap = new ConcurrentHashMap(); 默认构造函数会调用带三个参数的构造函数 1234567891011121314151617181920212223242526272829303132333435363738394041public ConcurrentHashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR, DEFAULT_CONCURRENCY_LEVEL);&#125;public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments //步骤① start int sshift = 0; int ssize = 1; while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; //步骤① end //步骤② start if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; //步骤② end // create segments and segments[0] //步骤③ start Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] this.segments = ss; //步骤③ end&#125; 上面定义了许多临时变量，注释写的又少，第一次看名字根本不知道这鬼东西代表什么意思，不过我们可以把已知的数据代进去，算出这些变量的值，再分析能不能找出一些猫腻。假设这是第一次默认创建： 步骤① concurrencyLevel = 16 ，可以计算出 sshift = 4，ssize = 16，segmentShift = 28，segmentMask = 15； 步骤② c = 16/16 = 1，cap = 2； 步骤③有句注释，创建 Segment 数组 segments 并初始化 segments [0] ，所以 s0 初始化后数组长度为2，负载因子0.75，阈值为1；再看这里的ss的初始化(重点，圈起来要考！！！)， ssize 此时为16，所以默认数组长度16，给人一种感觉正好和我们传的 concurrencyLevel 一样？看下下面的例子 例子1 例子2 ssize = 1，concurrencyLevel = 10 ssize = 1，concurrencyLevel = 8 ssize &lt;&lt;= 1 —&gt; 2&lt;10 满足 ssize &lt;&lt;= 1 —&gt; 2&lt;10 满足 ssize &lt;&lt;= 1 —&gt; 4&lt;10 满足 ssize &lt;&lt;= 1 —&gt; 4&lt;10 满足 ssize &lt;&lt;= 1 —&gt; 8&lt;10 满足 ssize &lt;&lt;= 1 —&gt; 8&lt;10 不满足 ssize = 8 ssize &lt;&lt;= 1 —&gt; 16&lt;10 不满足 ssize = 16 所以我们传 concurrencyLevel 不一定就是最后数组的长度，长度的计算公式： 长度 = 2的n次方(2的n次方 &gt;= concurrencyLevel) 到这里只是创建了一个长度为16的Segment 数组，并初始化数组0号位置，segmentShift和segmentMask还没派上用场，画图存档: 接着看 put 方法 12345678910111213141516public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; //步骤①注意valus不能为空！！！ if (value == null) throw new NullPointerException(); //根据key计算hash值，key也不能为null，否则hash(key)报空指针 int hash = hash(key); //步骤②派上用场了，根据hash值计算在segments数组中的位置 int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; //步骤③查看当前数组中指定位置Segment是否为空 //若为空，先创建初始化Segment再put值，不为空，直接put值。 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment s = ensureSegment(j); return s.put(key, hash, value, false);&#125; 步骤①可以看到和 HashMap 的区别，这里的 key/value 为空会报空指针异常；步骤②先根据 key 值计算 hash 值，再和前面算出来的两个变量计算出这个 key 应该放在哪个Segment中(具体怎么计算的有兴趣可以去研究下，先高位运算再取与)，假设我们算出来该键值对应该放在5号，步骤③判断5号为空，看下 ensureSegment() 方法 123456789101112131415161718192021222324252627282930private Segment&lt;K,V&gt; ensureSegment(int k) &#123; //获取segments final Segment&lt;K,V&gt;[] ss = this.segments; long u = (k &lt;&lt; SSHIFT) + SBASE; // raw offset Segment&lt;K,V&gt; seg; if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; //拷贝一份和segment 0一样的segment Segment&lt;K,V&gt; proto = ss[0]; // use segment 0 as prototype //大小和segment 0一致，为2 int cap = proto.table.length; //负载因子和segment 0一致，为0.75 float lf = proto.loadFactor; //阈值和segment 0一致，为1 int threshold = (int)(cap * lf); //根据大小创建HashEntry数组tab HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; //再次检查 if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // recheck 根据已有属性创建指定位置的Segment Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg; &#125; 该方法重点在于拷贝了segments[0]，因此新创建的Segment与segment[0]的配置相同，由于多个线程都会有可能执行该方法，因此这里通过UNSAFE的一些原子性操作的方法做了多次的检查，到目前为止画图存档： 现在“舞台”也有了，请开始你的表演，看下 Segment 的put方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; //步骤① start HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); //步骤① end V oldValue; try &#123; //步骤② start //获取Segment中的HashEntry[] HashEntry&lt;K,V&gt;[] tab = table; //算出在HashEntry[]中的位置 int index = (tab.length - 1) &amp; hash; //找到HashEntry[]中的指定位置的第一个节点 HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; //如果不为空，遍历这条链 if (e != null) &#123; K k; //情况① 之前已存过，则替换原值 if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; //情况② 另一个线程的准备工作 if (node != null) //链表头插入方式 node.setNext(first); else //情况③ 该位置为空，则新建一个节点(注意这里采用链表头插入方式) node = new HashEntry&lt;K,V&gt;(hash, key, value, first); //键值对数量+1 int c = count + 1; //如果键值对数量超过阈值 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) //扩容 rehash(node); else //未超过阈值，直接放在指定位置 setEntryAt(tab, index, node); ++modCount; count = c; //插入成功返回null oldValue = null; break; &#125; &#125; //步骤② end &#125; finally &#123; //步骤③ //解锁 unlock(); &#125; //修改成功，返回原值 return oldValue;&#125; 上面的 put 方法其实和 Java7 HashMap里大致是一样的，只是多了加锁/解锁两步，也正因为这样才保证了同一时刻只有一个线程拥有修改的权限。按步骤分析下上面的流程： 步骤① 执行 tryLock 方法获取锁，拿到锁返回null，没拿到锁执行 scanAndLockForPut 方法； 步骤② 和 HashMap 里的那一套思路是一样的，不理解可以看下之前的文章介绍(情况②下面介绍)； 步骤③ 执行 unLock 方法解锁 假设现在Thread1进来存值，前面没人来过，它可以成功拿到锁，根据计算，得出它要存的键值对应该放在HashEntry[] 的0号位置，0号位置为空，于是新建一个 HashEntry，并通过 setEntryAt() 方法，放在0号位置，然而还没等 Thread1 释放锁，系统的时间片切到了 Thread2 ，先画图存档 Thread2 也来存值，通过前面的计算，恰好 Thread2 也被定位到 segments[5]，接下来 Thread2 尝试获取锁，没有成功(Thread1 还未释放)，执行 scanAndLockForPut() 方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; //通过Segment和hash值寻找匹配的HashEntry HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; //重试次数 int retries = -1; // negative while locating node //循环尝试获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below //步骤① if (retries &lt; 0) &#123; //情况① 没找到，之前表中不存在 if (e == null) &#123; if (node == null) // speculatively create node //新建 HashEntry 备用,retries改成0 node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; //情况② 找到，刚好第一个节点就是，retries改成0 else if (key.equals(e.key)) retries = 0; //情况③ 第一个节点不是，移到下一个，retries还是-1，继续找 else e = e.next; &#125; //步骤② //尝试了MAX_SCAN_RETRIES次还没拿到锁，简直B了dog！ else if (++retries &gt; MAX_SCAN_RETRIES) &#123; //泉水挂机 lock(); break; &#125; //步骤③ //在MAX_SCAN_RETRIES次过程中，key对应的entry发生了变化，则从头开始 else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 通过上面的注释分析可以看出，Thread2 虽然此刻没有权限修改，但是它也没闲着，利用等锁的这个时间，把自己要放的键值对在数组中哪个位置计算出来了，这样当 Thread2 一拿到锁就可以立马定位到具体位置操作，节省时间。上面的步骤③稍微解释下，比如 Thread2 通过查找得知自己要修改的值在0号位置，但在 Thread1 里面又把该值改到了1号位置，如果它还去0号操作那肯定出问题了，所以需要重新确定。 假设 Thread2 put 值为(“亚索”，“98”)，对应1号位置，那么在 scanAndLockForPut 方法中对应情况①，画图存档： 再回到 Segment put 方法中的情况②，当 Thread1 释放锁后，Thread2 持有锁，并准备把亚索放在1号位置，然而此时 Segment[5] 里的键值对数量2 &gt; 阈值1，所以调用 rehash() 方法扩容， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172private void rehash(HashEntry&lt;K,V&gt; node) &#123; /* * Reclassify nodes in each list to new table. Because we * are using power-of-two expansion, the elements from * each bin must either stay at same index, or move with a * power of two offset. We eliminate unnecessary node * creation by catching cases where old nodes can be * reused because their next fields won't change. * Statistically, at the default threshold, only about * one-sixth of them need cloning when a table * doubles. The nodes they replace will be garbage * collectable as soon as they are no longer referenced by * any reader thread that may be in the midst of * concurrently traversing table. Entry accesses use plain * array indexing because they are followed by volatile * table write. */ //旧数组引用 HashEntry&lt;K,V&gt;[] oldTable = table; //旧数组长度 int oldCapacity = oldTable.length; //新数组长度为旧数组的2倍 int newCapacity = oldCapacity &lt;&lt; 1; //修改新的阈值 threshold = (int)(newCapacity * loadFactor); //创建新表 HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; //遍历旧表 for (int i = 0; i &lt; oldCapacity ; i++) &#123; HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; //确定在新表中的位置 int idx = e.hash &amp; sizeMask; //情况① 链表只有一个节点，指定转移到新表指定位置 if (next == null) // Single node on list newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; //情况② 扩容前后位置发生改变 int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; //将改变的键值对放到新表的对应位置 newTable[lastIdx] = lastRun; // Clone remaining nodes //情况③ 把链表中剩下的节点拷到新表中 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; //添加新的节点(链表头插入方式) int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 同样是扩容转移，这里的代码比 HashMap 中的 transfer 多了一些操作，在上上篇学习 HashMap 扩容可知，扩容后键值对的新位置要么和原位置一样，要么等于原位置+旧数组的长度，所以画个图来理解下上面代码这么写的原因： 前提：当前 HashEntry[] 长度为8，阈值为 8*0.75 = 6，所以 put 第7个键值对需要扩容 ，盖伦和亚索扩容前后位置不变，妖姬和卡特扩容后位置需要加上原数组长度，所以执行上面代码流程： 上面的代码先找出扩容前后需要转移的节点，先执行转移，然后再把该条链上剩下的节点转移，之所以这么写是起到复用的效果，注释中也说了，在使用默认阈值的情况下，只有大约 1/6 的节点需要被 clone 。注意到目前为止，可以看到无论是扩容转移还是新增节点，Java7都是采用的头插入方式，流程图如下： 相比之下，get 方法没有加锁/解锁的操作，代码比较简单就不分析了。 稍微说下Java8Java8 对比Java7有很大的不同，比如取消了Segments数组，允许并发扩容。 先看下ConcurrentHashMap的初始化 12public ConcurrentHashMap() &#123;&#125; 和Java7不一样，这里是个空方法，那么它具体的初始化操作呢？直接看下 put 方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485public V put(K key, V value) &#123; return putVal(key, value, false);&#125;/** Implementation for put and putIfAbsent */final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // key/value不能为空！！！ if (key == null || value == null) throw new NullPointerException(); //计算hash值 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; //注释① 表为null则初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); //CAS方法判断指定位置是否为null，为空则通过创建新节点，通过CAS方法设置在指定位置 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; //当前节点正在扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); //指定位置不为空 else &#123; V oldVal = null; //注释② 加锁 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; //节点是链表的情况 if (fh &gt;= 0) &#123; binCount = 1; //遍历整体链 for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; //如果已存在，替换原值 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果是新加节点，则以尾部插入实现添加 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; //节点是红黑树的情况 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; //遍历红黑树 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; else if (f instanceof ReservationNode) throw new IllegalStateException(\"Recursive update\"); &#125; &#125; if (binCount != 0) &#123; //链表中节点个数超过8转成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; &#125; &#125; //注释③ 添加节点 addCount(1L, binCount); return null;&#125; 代码有点长，第一次看很有可能引起身体不适，主要是因为引入了红黑树的判断和操作，以及线程安全的操作。同样key/value 为空会报空指针异常，这也是和 HashMap 一个明显的区别。 注释①调用 initTable 初始化数组 1234567891011121314151617181920212223242526private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // sizeCtl小于0，当前线程让出执行权 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin //CAS 操作将 sizeCtl 值改为-1 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\"unchecked\") //默认创建大小为16的数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; //初始化完再改回来 sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; put方法并没有加锁，那么它是如何保证创建新表的时候并发安全呢？答案就是这里的 sizeCtl ，sizeCtl 默认值为0，当一个线程初始化数组时，会将 sizeCtl 改成 -1，由于被 volatile 修饰，对于其他线程来说这个变化是可见的，上面代码看到后续线程判断 sizeCtl 小于0 就会让出执行权。 注释②Java8 摒弃了Segment，而是对数组中单个位置加锁。当指定位置节点不为 null 时，情况与 Java8 HashMap 操作类似，新节点的添加还是尾部插入方式。 注释③不管是链表的还是红黑树，确定之后总的节点数会加1，可能会引起扩容，Java8 ConcunrrentHashMap 支持并发扩容，之前扩容总是由一个线程将旧数组中的键值对转移到新的数组中，支持并发的话，转移所需要的时间就可以缩短了，当然相应的并发处理控制逻辑也就更复杂了，扩容转移通过 transfer 方法完成，Java8中该方法很长，感兴趣的可以看下源码。。。 用一个图来表示 Java8 ConcurrentHashMap的样子 总结通过分析源码对比了 HashMap 与 ConcurrentHashMap的差别，以及Java7和Java8上 ConcurrentHashMap 设计的不同，当然还有很多坑没有填，比如其中调用了很多UNSAFE的CAS方法，可以减少性能上的消耗，平时很少用，了解的比较少；以及红黑树的具体原理和实现，后续慢慢填。。。 转载自掘金HuYounger。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"图解HashMap(二)","date":"2019-03-20T12:45:07.000Z","path":"2019/03/20/图解HashMap(二)/","text":"概述上篇分析了HashMap的设计思想以及Java7和Java8源码上的实现，当然还有一些”坑”还没填完，比如大家都知道HashMap是线程不安全的数据结构，多线程情况下HashMap会引起死循环引用，它是怎么产生的？Java8引入了红黑树，那是怎么提高效率的？本篇先填第一个坑，还是以图解的形式加深理解。 Java7分析通过上一篇的整体学习，可以知道当存入的键值对超过HashMap的阀值时，HashMap会扩容，即创建一个新的数组，并将原数组里的键值对”转移”到新的数组中。在“转移”的时候，会根据新的数组长度和要转移的键值对key值重新计算在新数组中的位置。重温下Java7中负责”转移”功能的代码 123456789101112131415161718void transfer(Entry[] newTable, boolean rehash) &#123; //获取新数组的长度 int newCapacity = newTable.length; //遍历旧数组中的键值对 for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; //计算在新表中的索引，并到新数组中 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 为了加深理解，画个图如下 这里假设扩容前后5号坑石头、盖伦、蒙多的hash值与新旧数组长度取模运算后还是5。上篇文章也总结了，Java7扩容转移前后链表顺序会倒置。当只有单线程操作hashMap时，一切都是那么美好，但是如果多线程同时操作一个hashMap，问题就来了，下面看下多线程操作一个hashMap在Java7源码下是怎样引起死循环引用。 前戏是这样的：有两个线程分别叫Thread1和Thread2，它们都有操作同一个hashMap的权利，假设hashMap中的键值对是12个，石头和盖伦扩容前后的hash值与新旧数组长度取模运算后还是5。扩容前的模拟堆内存情况如图 Thread1得到执行权(Thread2被挂起)，Thread1往hashMap里put第13个键值对的时候判断超过阀值，执行扩容操作，Thread1创建了一个新数组，还没来得及转移旧键值对的时候，系统时间片反手切到Thread2(Thread1被挂起)，整个过程用图表示 可以看到Thread1只是创建了个新数组，还没来得及转移就被挂起了，新数组没有内容，此时在Thread1的视角认的是e是石头，next是盖伦；此时的模拟内存图情况 再看下Thread2的操作，同样Thread2往hashMap里put第13个键值对的时候判断超过阀值，执行扩容操作，Thread2先创建一个新数组，不同的是，Thread2运气好，在时间片轮换前转移工作也走完了。第一次遍历 第二次遍历 此时模拟的内存情况 可以看到此时对于盖伦来说，他的next是石头；对于石头来说，它的next为null，隐患就此埋下。接下来时间片又切到Thread1(停了半天终于轮到我出场了)，先看下Thread1的处境 结合代码分析如下 第一步： 第二步： 第三步： 第四步： 第五步： 第六步： 第七步： 第八步： 第九步： 第10步： 到这终于看到盖伦和石头”互指”，水乳交融。 那这会带来什么后果呢？后续操作新数组的5号坑会进入死循环(注意，操作其他坑并不会有问题)，例如Java7 put操作 Java7 get操作会执行getEntry，同样会引起死循环。 到此，Java7多线程操作HashMap可能形成死循环的原因剖析完成。 Java8分析通过上一篇的学习可知，Java7转移前后位置颠倒，而Java8转移键值对前后位置不变。同样的前戏，看下代码 此时模拟堆内存情况 Thread1的情况 这时候Thread2获得执行权，扩容并完成转移工作，通过上篇的学习可知，Java8在转移前会创建两条链表，即扩容后位置不加原数组长度的lo链和要加原数组长度的hi链，这里假设石头和盖伦扩容前后都在5号坑，即这是一条lo链(其实就算不在同一个坑也不影响，原因就是Java8扩容前后链顺序不变)。Thread2遍历第一次 第二次 可以看到Thread2全程是没有去修改石头和盖伦的引用关系，石头.next是盖伦，盖伦.next是null。那么Thread1得到执行权后其实只是重复了Thread2的工作。 总结通过源码分析，Java7在多线程操作hashmap时可能引起死循环，原因是扩容转移后前后链表顺序倒置，在转移过程中修改了原来链表中节点的引用关系；Java8在同样的前提下并不会引起死循环，原因是扩容转移后前后链表顺序不变，保持之前节点的引用关系。那是不是意味着Java8就可以把HashMap用在多线程中呢？个人感觉即使不会出现死循环，但是通过源码看到put/get方法都没有加同步锁，多线程情况最容易出现的就是：无法保证上一秒put的值，下一秒get的时候还是原值，建议使用ConcurrentHashMap。 感谢讲HashMap多线程死循环最详细的外国小哥 转载自掘金HuYounger。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"图解HashMap(一)","date":"2019-03-20T12:20:45.000Z","path":"2019/03/20/图解HashMap(一)/","text":"图解HashMap(一)概述HashMap是日常开发中经常会用到的一种数据结构，在介绍HashMap的时候会涉及到很多术语，比如时间复杂度O、散列(也叫哈希)、散列算法等，这些在大学课程里都有教过，但是由于某种不可抗力又还给老师了，在深入学习HashMap之前先了解HashMap设计的思路以及以及一些重要概念，在后续分析源码的时候就能够有比较清晰的认识。 HashMap是什么在回答这个问题之前先看个例子：小明打算从A市搬家到B市，拿了两个箱子把自己的物品打包就出发了。 到了B市之后，他想拿手机给家里报个平安，这时候问题来了，东西多了他忘记手机放在哪个箱子了？小明开始打开1号箱子找手机，没找到；再打开2号箱子找，找到手机。当只有2个箱子的时候，东西又不多的情况下，他可能花个2分钟就找到手机了，假如有20个箱子，每个箱子的东西又多又杂，那么花的时间就多了。小明总结了下查找耗时的原因，发现是因为这些东西放的没有规律，如果他把每个箱子分个类别，比如定一个箱子专门放手机、电脑等电子设备，有专门放衣服的箱子等等，那么他找东西花的时间就可以大大缩短了。 其实HashMap也是用到这种思路，HashMap作为一种数据结构，像数组和链表一样用于常规的增删改查，在存数据的时候(put)并不是随便乱放，而是会先做一次类似“分类”的操作再存储，一旦“分类”存储之后，下次取(get)的时候就可以大大缩短查找的时间。我们知道数组在执行查、改的效率很高，而增、删(不是尾部)的效率低，链表相反，HashMap则是把这两者结合起来，看下HashMap的数据结构。 从上面的结构可以看出，通常情况下HashMap是以数组和链表的组合构成(Java8中将链表长度超过8的链表转化成红黑树)。结合上面找手机的例子，我们简单分析下HashMap存取操作的心路历程。put存一个键值对的时候(比如存上图盖伦)，先根据键值”分类”，”分类”一顿操作后告诉我们，盖伦应该属于14号坑，直接定位到14号坑。接下来有几种情况： 14号坑没人，nice，直接存值； 14号有人，也叫盖伦，替换原来的攻击值； 14号有人，叫老王！插队到老王前面去(单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置) get取的时候也需要传键值，根据传的键值来确定要找的是哪个”类别”，比如找火男，”分类”一顿操作够告诉我们火男属于2号坑，于是我们直接定位到2号坑开始找，亚索不是…找到火男。 小结HashMap是由数组和链表组合构成的数据结构，Java8中链表长度超过8时会把长度超过8的链表转化成红黑树；存取时都会根据键值计算出”类别”(hashCode)，再根据”类别”定位到数组中的位置并执行操作。 HashCode是什么还是举个栗子：一个工厂有500号人，下图用两种方案来存储厂里员工的信件。 左右各有27个信箱，左边保安大哥存信的时候不做处理，想放哪个信箱就放哪个信箱，当员工去找信的时候，只好挨个信箱找，再挨个比对信箱里信封的名字，万一哥们脸黑，要找的放在最后一个信箱的最底下，悲剧…所以这种情况的时间复杂度为O(N)；右边采用HashCode的方式将27个信箱分类，分类的规则是名字首字母(第一个箱子放不写名字的哥们)，保安大哥将符合对应姓名的信件放在对应的信箱里，这样员工就不用挨个找了，只需要比对一个信箱里的信件即可，大大提高了效率，这种情况的时间复杂度趋于一个常数O(1)。 例子中右图其实就是hashCode的一个实现，每个员工都有自己的hashCode，比如李四的hashCode是L，王五的hashCode是W(这取决于你的hash算法怎么写)，然后我们根据确定的hashCode值把信箱分类，hashCode匹配则存在对应信箱。在Java的Object中可以调用hashCode()方法获取对象hashCode，返回一个int值。那么会出现两个对象的hashCode一样吗?答案是会的，就像上上个例子中盖伦和老王的hashCode就一样，这种情况网上有人称之为”hash碰撞”，出现这种所谓”碰撞”的处理上面已经介绍了解决思路，具体源码后续介绍。 小结hashCode是一个对象的标识，Java中对象的hashCode是一个int类型值。通过hashCode来指定数组的索引可以快速定位到要找的对象在数组中的位置，之后再遍历链表找到对应值，理想情况下时间复杂度为O(1)，并且不同对象可以拥有相同的hashCode。 HashMap的时间复杂度通过上面信箱找信的例子来讨论下HashMap的时间复杂度，在使用hashCode之后可以直接定位到一个箱子，时间的耗费主要是在遍历链表上，理想的情况下(hash算法写得很完美)，链表只有一个节点，就是我们要的。 那么此时的时间复杂度为O(1)，那不理想的情况下(hash算法写得很糟糕)，比如上面信箱的例子，假设hash算法计算每个员工都返回同样的hashCode 所有的信都放在一个箱子里，此时要找信就要依次遍历C信箱里的信，时间复杂度不再是O(1)，而是O(N)，因此HashMap的时间复杂度取决于算法的实现上，当然HashMap内部的机制并不像信箱这么简单，在HashMap内部会涉及到扩容、Java8中会将长度超过8的链表转化成红黑树，这些都在后续介绍。 小结HashMap的时间复杂度取决于hash算法，优秀的hash算法可以让时间复杂度趋于常数O(1)，糟糕的hash算法可以让时间复杂度趋于O(N)。 负载因子是什么我们知道HashMap中数组长度是16(什么？你说不知道，看下源码你就知道)，假设我们用的是最优秀的hash算法，即保证我每次往HashMap里存键值对的时候，都不会重复，当hashmap里有16个键值对的时候，要找到指定的某一个，只需要1次； 之后继续往里面存值，必然会发生所谓的”hash碰撞”形成链表，当hashmap里有32个键值对时，找到指定的某一个最坏情况要2次；当hashmap里有128个键值对时，找到指定的某一个最坏情况要8次 随着hashmap里的键值对越来越多，在数组数量不变的情况下，查找的效率会越来越低。那怎么解决这个问题呢？只要增加数组的数量就行了，键值对超过16，相应的就要把数组的数量增加(HashMap内部是原来的数组长度乘以2)，这就是网上所谓的扩容，就算你有128个键值对，我们准备了128个坑，还是能保证”一个萝卜一个坑”。 其实扩容并没有那么风光，就像ArrayList一样，扩容是件很麻烦的事情，要创建一个新的数组，然后把原来数组里的键值对”放”到新的数组里，这里的”放”不像ArrayList那样用原来的index，而是根据新表的长度重新计算hashCode，来保证在新表的位置，老麻烦了，所以同一个键值对在旧数组里的索引和新数组中的索引通常是不一致的(火男：”我以前是3号，怎么现在成了127号，给我个完美的解释！”新表：”大清亡了，现在你得听我的”)。另外，我们也可以看出这是典型的以空间换时间的操作。 说了这么多，那负载因子是个什么东西？负载因子其实就是规定什么时候扩容。上面我们说默认hashmap数组大小为16，存的键值对数量超过16则进行扩容，好像没什么毛病。然而HashMap中并不是等数组满了(达到16)才扩容，它会存在一个阀值(threshold)，只要hashmap里的键值对大于等于这个阀值，那么就要进行扩容。阀值的计算公式： 阀值 = 当前数组长度✖负载因子 hashmap中默认负载因子为0.75，默认情况下第一次扩容判断阀值是16 ✖ 0.75 = 12；所以第一次存键值对的时候，在存到第13个键值对时就需要扩容了；或者另外一种理解思路：假设当前存到第12个键值对：12 / 16 = 0.75，13 / 16 = 0.8125(大于0.75需要扩容) 。肯定会有人有疑问，我要这铁棒有何用？不，我要这负载因子有何用?直接规定超过数组长度再扩容不就行了，还省得每次扩容之后还要重新计算新的阀值，Google说取0.75是一个比较好的权衡，当然我们可以自己修改，HashMap初识化时可以指定数组大小和负载因子，你完全可以改成1。 1public HashMap(int initialCapacity, float loadFactor) 我的理解是这负载因子就像人的饭量，有的人吃要7分饱，有的人要10分饱，稳妥起见默认让我们7.5分饱。 小结在数组大小不变的情况下，存放键值对越多，查找的时间效率会降低，扩容可以解决该问题，而负载因子决定了什么时候扩容，负载因子是已存键值对的数量和总的数组长度的比值。默认情况下负载因子为0.75，我们可在初始化HashMap的时候自己修改。 hash与Rehashhash和rehash的概念其实上面已经分析过了，每次扩容后，转移旧表键值对到新表之前都要重新rehash，计算键值对在新表的索引。如下图火男这个键值对被存进hashmap到后面扩容，会经过hash和rehash的过程 第一次hash可以理解成’”分类”‘，方便后续取、改等操作可以快速定位到具体的”坑”。那么为什么要进行rehash，按照之前元素在数组中的索引直接赋值，例如火男之前3号坑，现在跑到30号坑。 个人理解是，在未扩容前，可以看到如13号链的长度是3，为了保证我们每次查找的时间复杂度O趋于O(1)，理想的情况是”一个萝卜一个坑”，那么现在”坑”多了，原来”3个萝卜一个坑”的情况现在就能有效的避免了。 源码分析Java7源码分析先看下Java7里的HashMap实现，有了上面的分析，现在在源码中找具体的实现。 123456789101112131415161718192021222324252627282930313233343536//HashMap里的数组transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;//Entry对象，存key、value、hash值以及下一个节点static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; Entry&lt;K,V&gt; next; int hash;&#125;//默认数组大小，二进制1左移4位为16static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4；//负载因子默认值static final float DEFAULT_LOAD_FACTOR = 0.75f; //当前存的键值对数量transient int size; //阀值 = 数组大小 * 负载因子int threshold;//负载因子变量final float loadFactor;//默认new HashMap数组大小16，负载因子0.75public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;//可以指定数组大小和负载因子public HashMap(int initialCapacity, float loadFactor) &#123; //省略一些逻辑判断 this.loadFactor = loadFactor; threshold = initialCapacity; //空方法 init();&#125; 以上就是HashMap的一些先决条件，接着看平时put操作的代码实现，put的时候会遇到3种情况上面已分析过，看下Java7代码： 123456789101112131415161718192021222324252627282930public V put(K key, V value) &#123; //数组为空时创建数组 if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; //key为空单独对待 if (key == null) return putForNullKey(value); //①根据key计算hash值 int hash = hash(key); //②根据hash值和当前数组的长度计算在数组中的索引 int i = indexFor(hash, table.length); //遍历整条链表 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; //③情况1.hash值和key值都相同的情况，替换之前的值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); //返回被替换的值 return oldValue; &#125; &#125; modCount++; //③情况2.坑位没人,直接存值或发生hash碰撞都走这 addEntry(hash, key, value, i); return null; &#125; 先看上面key为空的情况(上面画图的时候总要在第一格留个空key的键值对)，执行 putForNullKey() 方法单独处理，会把该键值对放在index0，所以HashMap中是允许key为空的情况。再看下主流程： 步骤①.根据键值算出hash值 — &gt; hash(key) 步骤②.根据hash值和当前数组的长度计算在数组中的索引 — &gt; indexFor(hash, table.length) 123456static int indexFor(int h, int length) &#123; //hash值和数组长度-1按位与操作，听着费劲？其实相当于h%length;取余数(取模运算) //如：h = 17，length = 16;那么算出就是1 //&amp;运算的效率比%要高 return h &amp; (length-1);&#125; 步骤③情况1.hash值和key值都相同，替换原来的值，并将被替换的值返回。 步骤③情况2.坑位没人或发生hash碰撞 — &gt; addEntry(hash, key, value, i) 123456789101112void addEntry(int hash, K key, V value, int bucketIndex) &#123; //当前hashmap中的键值对数量超过阀值 if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; //扩容为原来的2倍 resize(2 * table.length); hash = (null != key) ? hash(key) : 0; //计算在新表中的索引 bucketIndex = indexFor(hash, table.length); &#125; //创建节点 createEntry(hash, key, value, bucketIndex);&#125; 如果put的时候超过阀值，会调用 resize() 方法将数组大小扩大为原来的2倍，并且根据新表的长度计算在新表中的索引(如之前17%16 =1，现在17%32=17)，看下resize方法 12345678910111213141516171819void resize(int newCapacity) &#123; //传入新的容量 //获取旧数组的引用 Entry[] oldTable = table; int oldCapacity = oldTable.length; //极端情况，当前键值对数量已经达到最大 if (oldCapacity == MAXIMUM_CAPACITY) &#123; //修改阀值为最大直接返回 threshold = Integer.MAX_VALUE; return; &#125; //步骤①根据容量创建新的数组 Entry[] newTable = new Entry[newCapacity]; //步骤②将键值对转移到新的数组中 transfer(newTable, initHashSeedAsNeeded(newCapacity)); //步骤③将新数组的引用赋给table table = newTable; //步骤④修改阀值 threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 上面的重点是步骤②，看下它具体的转移操作 123456789101112131415161718void transfer(Entry[] newTable, boolean rehash) &#123; //获取新数组的长度 int newCapacity = newTable.length; //遍历旧数组中的键值对 for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; //计算在新表中的索引，并到新数组中 int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 这段for循环的遍历会使得转移前后键值对的顺序颠倒(Java7和Java8的区别)，画个图就清楚了，假设石头的key值为5，盖伦的key值为37,这样扩容前后两者还是在5号坑。第一次： 第二次 最后再看下创建节点的方法 12345void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; 创建节点时，如果找到的这个坑里面没有存值，那么直接把值存进去就行了，然后size++；如果是碰撞的情况， 前面说的以单链表头插入的方式就是这样(盖伦：”老王已被我一脚踢开！“)，总结一下Java7 put流程图 相比put，get操作就没这么多套路，只需要根据key值计算hash值，和数组长度取模，然后就可以找到在数组中的位置(key为空同样单独操作)，接着就是遍历链表，源码很少就不分析了。 Java8源码分析基本思路是一样的 123456789//定义长度超过8的链表转化成红黑树static final int TREEIFY_THRESHOLD = 8;//换了个马甲还是认识你！！！static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next;&#125; 看下Java8 put的源码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public V put(K key, V value) &#123; //根据key计算hash值 return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //步骤1.数组为空或数组长度为0，则扩容(咦，看到不一样咯) if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //步骤2.根据hash值和数组长度计算在数组中的位置 //如果\"坑\"里没人，直接创建Node并存值 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; //步骤3.\"坑\"里有人，且hash值和key值都相等，先获取引用，后面会用来替换值 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //步骤4.该链是红黑树 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //步骤5.该链是链表 else &#123; for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; //步骤5.1注意这个地方跟Java7不一样，是插在链表尾部！！！ p.next = newNode(hash, key, value, null); //链表长度超过8，转化成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //步骤5.2链表中已存在且hash值和key值都相等，先获取引用，后面用来替换值 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //统一替换原来的值 e.value = value; afterNodeAccess(e); //返回原来的值 return oldValue; &#125; &#125; ++modCount; //步骤6.键值对数量超过阀值，扩容 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 通过上面注释分析，对比和Java7的区别，Java8一视同仁，管你key为不为空的统一处理，多了一步链表长度的判断以及转红黑树的操作，并且比较重要的一点，新增Node是插在尾部而不是头部！！！。当然上面的主角还是扩容resize操作 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788final Node&lt;K,V&gt;[] resize() &#123; //旧数组的引用 Node&lt;K,V&gt;[] oldTab = table; //旧数组长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; //旧数组阀值 int oldThr = threshold; //新数组长度、新阀值 int newCap, newThr = 0; if (oldCap &gt; 0) &#123; //极端情况，旧数组爆满了 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //阀值改成最大，放弃治疗直接返回旧数组 threshold = Integer.MAX_VALUE; return oldTab; &#125; //扩容咯，这里采用左移运算左移1位，也就是旧数组*2 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) //同样新阀值也是旧阀值*2 newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; //初始化在这里 else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //更新阀值 threshold = newThr; @SuppressWarnings(&#123;\"rawtypes\",\"unchecked\"&#125;) //创建新数组 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; //遍历旧数组，把原来的引用取消，方便垃圾回收 oldTab[j] = null; //这个链只有一个节点，根据新数组长度计算在新表中的位置 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //红黑树的处理 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //链表长度大于1，小于8的情况，下面高能，单独拿出来分析 else &#123; // preserve order Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; 可以看到，Java8把初始化数组和扩容全写在resize方法里了，但是思路还是一样的，扩容后要转移，转移要重新计算在新表中的位置，上面代码最后一块高能可能不太好理解，刚开始看的我一脸懵逼，看了一张美团博客的分析图才豁然开朗，在分析前先捋清楚思路 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1(5)和key2(21)两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 图a中key1(5)和key(21)计算出来的都是5，元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 图b中计算后key1(5)的位置还是5，而key2(21)已经变成了21，因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”。 有了上面的分析再回来看下源码 123456789101112131415161718192021222324252627282930313233343536else &#123; // preserve order //定义两条链 //原来的hash值新增的bit为0的链，头部和尾部 Node&lt;K,V&gt; loHead = null, loTail = null; //原来的hash值新增的bit为1的链，头部和尾部 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; //循环遍历出链条链 do &#123; next = e.next; if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); //扩容前后位置不变的链 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; //扩容后位置加上原数组长度的链 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125;&#125; 为了更清晰明了，还是举个栗子，下面的表定义了键和它们的hash值(数组长度为16时，它们都在5号坑) Key Hash 石头 5 盖伦 5 蒙多 5 妖姬 21 狐狸 21 日女 21 假设一个hash算法刚好算出来的的存储是这样的，在存第13个元素时要扩容 那么流程应该是这样的(只关注5号坑键值对的情况)，第一次： 第二次： 省略中间几次，第六次 两条链找出来后，最后转移一波，大功告成。 12345678910//扩容前后位置不变的链if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead;&#125;//扩容后位置加上原数组长度的链if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead;&#125; 总结下Java8 put流程图 对比1.发生hash冲突时，Java7会在链表头部插入，Java8会在链表尾部插入 2.扩容后转移数据，Java7转移前后链表顺序会倒置，Java8还是保持原来的顺序 3.关于性能对比可以参考美团技术博客，引入红黑树的Java8大程度得优化了HashMap的性能 转载自掘金HuYounger。","tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"数据结构--链表","date":"2019-03-18T10:54:36.000Z","path":"2019/03/18/数据结构-链表/","text":"","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"G1垃圾收集器","date":"2019-03-13T12:47:14.000Z","path":"2019/03/13/G1垃圾收集器/","text":"G1是一款面向服务端应用的垃圾收集器。G1最大的特点是引入分区的思路，弱化了分代的概念，合理利用垃圾收集各个周期的资源，解决了其他收集器甚至CMS的众多缺陷。 之前介绍的几组垃圾收集器组合，都有几个共同点： 年轻代、老年代是独立且连续的内存块； 年轻代收集使用单Eden、双survivor进行复制算法； 老年代收集必须扫描整个老年代区域； 都是以尽可能少而快的执行GC为设计原则。 开启参数：-XX:+UseG1GC","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"CMS垃圾收集器","date":"2019-03-13T12:47:02.000Z","path":"2019/03/13/CMS垃圾收集器/","text":"CMS垃圾收集器（Concurrent Mark Sweep）,CMS非常适合堆内存大、CPU核数多的服务器端应用，也是G1出现之前大型应用的首选收集器。 特点针对老年代，基于“标记-清除”算法(不进行压缩操作，产生内存碎片)，以获取最短回收停顿时间为目标，并发收集、低停顿，HotSpot在JDK1.5推出的第一款真正意义上的并发（Concurrent）收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。 应用场景与用户交互较多的场景，希望系统停顿时间最短，注重服务的响应速度，以给用户带来较好的体验，如常见WEB、B/S系统的服务器上的应用。 运作过程 初始标记：仅标记一下GC Roots能直接关联到的对象，速度很快，但需要“Stop The World”。多线程执行。 并发标记：进行GC Roots Tracing的过程，刚才产生的集合中标记出存活对象，应用程序也在运行，并不能保证可以标记出所有的存活对象。 重新标记：为了修正并发标记期间因用户程序继续运作而导致标记变动的那一部分对象的标记记录，需要“Stop The World”，且停顿时间比初始标记稍长，但远比并发标记短，采用多线程并行执行来提升效率。 并发清除：回收所有的垃圾对象。 整个过程中耗时最长的并发标记和并发清除都可以与用户线程一起工作，所以总体上说，CMS收集器的内存回收过程与用户线程一起并发执行。 CMS收集器运行示意图如下： 缺点 对CPU资源非常敏感 并发收集虽然不会暂停用户线程，但因为占用一部分CPU资源，还是会导致应用程序变慢，总吞吐量降低。 CMS的默认收集线程数量是=(CPU数量+3)/4； 当CPU数量多于4个，收集线程占用的CPU资源多于25%，对用户程序影响可能较大；不足4个时，影响更大，可能无法接受，为了应付这种情况，虚拟机提供了一种称为“增量式并发收集器”的CMS收集器变种。 无法处理浮动垃圾 在并发清除时，用户线程新产生的垃圾，称为浮动垃圾。这使得并发清除时需要预留一定的内存空间，不能像其他收集器在老年代几乎填满再进行收集，也可以认为CMS所需要的空间比其他垃圾收集器大。 -XX:CMSInitiatingOccupancyFraction：设置CMS预留内存空间，JDK1.5默认值为68%，JDK1.6变为大约92%。 可能出现“Concurrent Mode Failure”失败 如果CMS预留内存空间无法满足程序需要，就会出现一次“Concurrent Mode Failure”失败，这时JVM启用后备预案：临时启用Serail Old收集器，而导致另一次Full GC的产生，这样的代价是很大的，所以CMSInitiatingOccupancyFraction不能设置得太大。 产生大量内存碎片 由于CMS基于“标记-清除”算法，清除后不进行压缩操作。 解决方法： 1.-XX:+UseCMSCompactAtFullCollection，使得CMS出现上面这种情况时不进行Full GC，而开启内存碎片的合并整理过程，但合并整理过程无法并发，停顿时间会变长，默认开启（但不会进行，结合下面的CMSFullGCsBeforeCompaction）。 2.-XX:+CMSFullGCsBeforeCompaction，设置执行多少次不压缩的Full GC后，来一次压缩整理，为减少合并整理过程的停顿时间。默认为0，也就是说每次都执行Full GC，不会进行压缩整理，由于空间不再连续，CMS需要使用可用“空闲列表”内存分配方式，这比简单实用“碰撞指针”分配内存消耗大。 总体来看，与Parallel Old垃圾收集器相比，CMS减少了执行老年代垃圾收集时应用暂停的时间，但却增加了新生代垃圾收集时应用暂停的时间、降低了吞吐量而且需要占用更大的堆空间。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"HotSpot垃圾收集器","date":"2019-03-13T12:10:13.000Z","path":"2019/03/13/HotSpot垃圾收集器/","text":"上图展示了7种收集器，白色区域为新生代收集器，灰色区域为老年代收集器。如果两个收集器之间存在连线，就说明它们可以搭配使用。 Serial收集器 Serial收集器是一个新生代收集器，单线程收集器，使用复制算法，单线程指不仅仅只会使用一个CPU或者一条收集线程去完成垃圾收集，更重要的是它在进行垃圾收集时，必须暂停其他所有线程的工作。（Stop-the-world） Serial收集器是虚拟机运行在Client模式下的默认新生代收集器。其优点：简单而高效（与其他收集器的单线程比），对于限定单个CPU的环境来说，Serial收集器减少了线程交互的开销。 所以运行在Client模式下的虚拟机来说是一个不错的选择。 缺点：JVM在后台自动发起和自动完成的，把用户正常的工作线程全部停掉，即GC停顿，会带给用户不良的体验。 ParNew 收集器 Serial收集器的多线程版本，除了使用多条线程进行垃圾收集之外，其余行为与Serial收集器一样；实现上，也共用了相当多的代码。但是它确实许多运行在Server模式下虚拟机中首选的新生代收集器，其中一个与性能无关的原因就是除了Serial收集器外，目前只有ParNew收集器能与CMS收集器配合工作。 ParNew 收集器也是使用 -XX:+UseConcMarkSeepGC选项后的默认新生代收集器，也可以使用-XX:UseParNewGC选项来强制指定;ParaNew在单cpu的环境绝对不会比Serial收集效果好,在多线程环境下更适用于ParaNew，默认开启的垃圾收集器线程数就是CPU数，可通过-XX：parallelGCThreads参数来限制收集器线程数 Parallel Scavenge收集器 Parallel Scavenge收集器也是一个新生代收集器，它也是使用复制算法的收集器，又是并行多线程收集器。parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而parallel Scavenge收集器的目标则是达到一个可控制的吞吐量(Throughput)。就是CPU用于运行用户代码时间与CPU总消耗时间比值，即吞吐量=运行用户代码时间/(用户代码时间+GC时间)。虚拟机总共运行了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。 Parallel Scavenge收集器提供两个参数用于精确控制吞吐量，分别是最大垃圾收集时间-XX:MaxGCPauseMillis和直接设置吞吐量大小-XX:GCTimeRatio。 MaxGCPauseMillis允许设置一个大于0的毫秒数，收集器尽可能的保证GC时间不超过此设定值，注意千万不要认为把这个参数设置的小就能让系统收集速度变快，GC停顿时间缩短是以减少吞吐量的代价和新生代空间的代价来换取的。也就是这个值越小，新生代空间也会越小，也就会导致垃圾收集更加频繁。 GCTimeRatio参数值允许设置为一个大于0小于100的整数，也是GC时间的占比数，例如 -XX:GCTimeRatio=19，GC时间占比=(1/19+1)即5%，默认为99即1%。 Scavenge收集器还提供一个参数-XX:+UseAdaptiveSizePolicy 。这是一个开关参数，当使用这个参数时就不需要知道新生代的大小、Eden与Survivor区的比例、晋升老年代对象年龄等细节参数，虚拟机会根据系统运行性能监控信息，动态调整这些参数以提供最合适的GC时间或者最大吞吐量，这种GC自适应的调节策略(GC Ergonomics)也是和ParNew很大的区别。 停顿时间越短就越适合需要与用户交互的程序，良好的响应速度能提升用户体验，而高吞吐量则可用高效率地利用CPU时间，尽快完成程序的运算任务，主要适合在后台运算而不需要太多交互的任务。 Serial Old 收集器 Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是在于给Client模式下的虚拟机使用。主要两大用途： （1）在JDK1.5以及之前的版本中与Parallel Scavenge收集器搭配使用 （2）作为CMS收集器的后备预案，在并发收集发生Concurrent Mode Failure时使用 Parallel Old收集器 Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法，在JDK1.6才开始提供的。在此之前，Parallel Scavenge 一直处于一个很尴尬的状态，如果新生代选择了Paralle Scavenge，老年代只能选择Serial Old，因为CMS无法于Parallel Scavenge配合工作，由于Serial Old的服务端性能上的欠缺，就算使用Paraller Scavenge在吞吐量上还是没有PraNew+CMS组合高。直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的应用组合(Paraller Scavenge + Parallel Old)。 应用场景：在注重吞吐量以及CPU资源敏感的场合，都可以优先考虑Parallel Scavenge+Parallel Old收集器 设置参数： “-XX:+UseParallelOldGC”：指定使用Parallel Old收集器。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"垃圾回收算法","date":"2019-03-13T11:30:47.000Z","path":"2019/03/13/垃圾回收算法/","text":"判断对象是否存活1.引用计数法 给每个对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1，引用失效时，引用就减1。任何时刻计数器为0的对象就是不可能再被使用的对象，此时该对象为可回收对象。 优点： 实现简单，判定效率高。 缺点： 无法解决对象之间相互循环引用的问题。 123456789101112131415161718public class CountTest&#123; public Object instance = null; private static final int _1MB = 1024*1024; private byte[] bigSize = new byte[2*_1MB]; public static void main(String[] args)&#123; CountTest count1 = new CountTest(); CountTest count2 = new CountTest(); count1.instance = count2; count2.instance = count1; count1 = null; count2 = null; System.gc(); &#125;&#125; 实际上这两个对象已经不可能再被引用，但是因为它们之间相互引用，导致它们的计数器都不为0，因此GC无法回收它们。 2.可达性分析法 该算法的基本思想是通过一系列的成为“GC roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（reference chain），当一个对象到GC roots没有任何引用链相连时，则证明此对象是不可用的。 在Java语言中，可作为GC roots的对象包括以下几种： 虚拟机栈（栈帧中的本地变量表）中引用的对象； 方法区中类静态属性引用的对象； 方法区中常量引用的对象； 本地方法栈中JNI（即native方法）引用的对象。 即使在可达性分析算法中不可达的对象，也并非是“非死不可”的，这时它处于“缓刑”阶段，要正真宣告一个对象“死亡”，至少要经历两次标记过程。如果对象在经历可达性分析后发现没有与GC roots的引用链，那它将会被第一次标记并进行一次筛选，筛选的条件是此对象是否有必要执行finalize()方法，当对象没有覆盖finalize()方法或者finalize()方法已经被虚拟机调用过，则虚拟机将这两种情况视为“没有必要执行”。 如果该对象被判定为有必要执行finalize()方法，那么这个对象将会被放到一个叫做F-Queue的队列中，并在稍后在一个由虚拟机自动建立的低优先级的Finalizer线程负责运行，但是虚拟机并不“承诺”会等待它执行结束。finalize()方法是对象逃脱死亡命运的最后一次机会。意思就是重写finalize()方法，譬如把自己（this关键字）赋值给某个类变量或者对象的成员变量，那在第二次标记时它将被移除出“即将回收”的集合；如果对象这时候还没有逃脱，那基本上它就真的被回收了。但是只能自救一次，因为finalize()方法只会被系统调用一次。 垃圾回收算法标记清除算法（Mark-Sweep）首先标记处所有需要回收的对象，在标记完成后统一回收所有被标记的对象。 缺点： 效率低下，标记和清除两个过程效率都相对较低； 标记清除之后会产生大量不连续的内存碎片，内存碎片太多可能会导致以后在程序需要创建较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法（Copying）将可用内存按容量划分为大小相等的两块，每次只是用其中的一块，当这一块的内存用完了，就将还存活的对象复制到另一块上面，然后把用过的内存空间一次性清理掉。 优点：每次都是对整个内存半区进行垃圾回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效； 缺点：内存利用率低，这种算法的代价是将内存缩小为了原来的一半，代价太大。 标记整理算法（Mark-Compact）标记整理算法，标记过程与标记清除算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉边界以外的内存。 分代收集算法（Generation Collection）根据对象存活周期的不同，将内存划分为几块。一般把java堆划分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的算法。 在新生代中，每次垃圾收集都发现有大批对象死去，只有少量存活，那就采用复制法，只需要付出少量存活对象的复制成本就可以完成收集。 而老年代中对象存活率高，没有额外空间对它进行分配担保，就必须采用“标记-清除”算法或“标记-整理”算法来进行回收。 增量算法（待补充）如果一次性将所有垃圾进行处理，需要系统长时间的停顿，那么就让垃圾回收的线程和应用程序的线程交替执行。垃圾回收只是回收一小块内存，接着切换到应用程序线程。这样就减少了系统的停顿时间。因为线程的切换和上下文的转换的消耗，会使得垃圾回收的总体成本上升，造成系统吞吐量下降。增量收集算法的技术是标记清除和整理，只是允许垃圾回收进程以阶段完成标记、清理、或复制工作。 RC Immix算法（待补充）","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"}]},{"title":"Excel小技巧","date":"2019-03-12T12:50:14.000Z","path":"2019/03/12/Excel小技巧/","text":"1.复制多行 A B C 1111 1111 2222 1111 3333 2222 2222 3333 3333 如何将列A变成列C那样，每行数据重复N次。 在列3输入公式 =INDEX(A:A,ROW(A2)/2)。 A:A，指的是取A的数据，如果是E列，就E:E。 ROW(A2)/2是重复两次，如果是要重复N次，就ROW(AN)/N。原理没了解，大概是取模吧。 2.长度补足 A B C 1 00001 2 00002 3 00003 4 00004 如何将列A变成列C那样，每行数据都保持N位数，长度不足用0补齐。 在列C输入公式 =REPT(0,5-LEN(A2))&amp;A2 0：指的是需要重复的文本 5：是你需要的固定长度 REPT(0,5-LEN(A2))就是假设单元格A2文本长度为X，那么就补齐5-X的0，最后&amp;A2就是将之前A2的数据拼到后面。","tags":[{"name":"日常","slug":"日常","permalink":"http://yoursite.com/tags/日常/"}]},{"title":"PowerShell","date":"2019-03-12T12:40:14.000Z","path":"2019/03/12/PowerShell/","text":"1.字符串查找 在当前目录查找指定字符串，不递归查询（即不查子文件夹） select-string -path 路径\\* -pattern “字符串” 例如查询D:\\hexo\\source_posts目录下包含“666”的文件 select-string -path D:\\hexo\\source_posts\\* -pattern “666” 如果只查询txt文件 select-string -path D:\\hexo\\source_posts\\*.txt -pattern “666” 如果当前目录有子文件夹，可能会报错，不用管，是因为访问不了子文件夹。 在当前目录查找指定字符串，递归查询（即所有子文件夹也要查询） get-childitem -path 路径 -recurse | select-string -pattern “字符串” -allmatches 参数 -recurse 递归查询 -allmatches 匹配所有，取消该参数，只匹配每个文件第一个符合的地方 get-childitem -path D:\\hexo\\source_posts -recurse | select-string -pattern “666” -allmatches","tags":[{"name":"日常","slug":"日常","permalink":"http://yoursite.com/tags/日常/"}]},{"title":"JVM局部变量表","date":"2019-03-12T11:47:14.000Z","path":"2019/03/12/JVM局部变量表/","text":"","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"MySql慢日志查询","date":"2019-03-07T11:30:14.000Z","path":"2019/03/07/MySql慢日志查询/","text":"","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"数据库基本概念","date":"2019-03-06T14:30:14.000Z","path":"2019/03/06/数据库基本概念/","text":"本文以MySql的InnoDB引擎为例。 数据库基本概念InnoDB存储引擎结构与Oracle大致相同，所有数据都被逻辑地存放在一个空间中，我们称之为表空间（tablespace）。表空间又由段（segment）、区（extent）、页（page）组成。页有时也被称为块（block）。 对于InnoDB而言，其数据文件最小的存储单位为页。默认为16KB大小。InnoDB存储引擎对于空间的申请不是每次以16KB的方式申请，而是以区的方式。一个区的大小为1MB，总共有64个页。这样的目的是提高空间的申请效率。如果数据是按照减值顺序存放的，那么读取这些页，将在一个连续的地址中，这样可以避免磁头的大量的寻址时间。 行行对应的是表中的行记录，每页存储最多的行记录也是有硬性规定的，最多16KB/2-200，即7992行（16KB是页大小） 页1.数据库I/O的基本单位，16KB，存储引擎的最小管理单位 2.就是MySql的聚簇索引的叶子节点 3.物理上64个连续的页可以连成一个区 4.数据页、Undo页、系统页、事务数据页、插入缓冲位图页、插入缓冲空闲列表页、未压缩的二进制大对象页、压缩的二进制大对象页 5.从InnoDB 1.2.x版本开始，可以利用innodb_page_size来改变页size，但是改变只能在初始化InnoDB实例前进行修改，之后便无法进行修改，除非mysqldump导出创建新库 区1.按区分配存储空间 2.区是InnoDB存储引擎访问的最小单位 3.区是InnoDB寻找存储空间的最小单位 4.一个区由64个连续的页组成，大小为1MB，为保证区中页的连续性，InnoDB会一次从磁盘中申请4~5个区 5.新建表的时候，空表的默认大小为96KB,是由于为了高效的利用磁盘空间，在开始插入数据时表会先利用32个页大小的碎片页来存储数据，当这些碎片使用完后，表大小才会按照MB倍数来增加 段1.每个用户至少有两个段 2.在MySql中，数据是按照B+树来存储，因此数据即索引，因此数据段是B+树的叶子节点，索引段是B+树的非叶子节点 3.回滚段即rollback segment，管理undo log segment，用于存储undo日志，用于事务失败后数据回滚以及在事务未提交之前通过undo日志获取之前版本的数据，在InnoDB1.1版本之前一个InnoDB,只支持一个回滚段，支持1023个并发修改事务同时进行，在InnoDB1.2版本，将回滚段数量提高到了128个，也就是说可以同时进行128*1023个并发修改事务。 表空间1.表空间是InnoDB存储引擎结构的最高层，所有的数据都存放在表空间中。 2.默认只有一个表空间ibdata1（系统表空间，只能扩大不能缩小） 3.默认使用共享表空间存储方式 4.单独表空间存储方式，开启独立表空间的参数：innodb_file_per_table=1，此时每个表的数据以一个单独的文件来存放 表碎片及表空间碎片的判断及清除临时表空间","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"索引工作原理","date":"2019-03-04T11:30:14.000Z","path":"2019/03/04/索引工作原理/","text":"聚簇索引每个InnoDB的表都拥有一个索引，称之为聚簇索引，此索引中存储着行记录，一般来说，聚簇索引是根据主键生成的。为了能够获得高性能的查询、插入和其他数据库操作，理解InnoDB聚簇索引是很有必要的。 聚簇索引按照如下规则创建： 当定义了主键后，InnoDB会利用主键来生成其聚簇索引； 如果没有主键，InnoDB会选择一个非空的唯一索引来创建聚簇索引； 如果这也没有，InnoDB会隐式的创建一个自增的列来作为聚簇索引。 聚簇索引整体是一个b+树，非叶子节点存放的是键值，叶子节点存放的是行数据，称之为数据页，这就决定了表中的数据也是聚簇索引中的一部分，数据页之间是通过一个双向链表来链接的，B+树是一棵平衡查找树，也就是聚簇索引的数据存储是有序的，但是这个是逻辑上的有序，事实上物理存储可以隔很远，因为数据页之间是通过双向链表来连接，假如物理存储是顺序的话，那维护聚簇索引的成本非常的高。 辅助索引除了聚簇索引之外的索引都可以称之为辅助索引，与聚簇索引的区别在于辅助索引的叶子节点中存放的是主键的键值。一张表可以存在多个辅助索引，但是只能有一个聚簇索引。 通过辅助索引来查找对应的行记录的话，需要进行两步，第一步通过辅助索引来确定对应的主键，第二步通过相应的主键值在聚簇索引中查询到对应的行记录，也就是进行两次B+树搜索。相反通过辅助索引来查询主键的话，遍历一次辅助索引就可以确定主键了，也就是所谓的索引覆盖，不用回表（不用查询聚簇索引）。 创建辅助索引，可以创建单列的索引，也就是用一个字段来创建索引，也可以用多个字段来创建副主索引称为联合索引。 辅助索引还有一个概念便是索引覆盖，索引覆盖的一个好处便是辅助索引不含行记录，因此其大小远远小于聚簇索引，利用辅助索引进行查询可以减少大量的IO操作。 创建索引的原则最左原则参考资料：《高性能 MySQL 第三版》","tags":[{"name":"数据库","slug":"数据库","permalink":"http://yoursite.com/tags/数据库/"}]},{"title":"数据结构--二叉树","date":"2019-03-03T11:30:14.000Z","path":"2019/03/03/数据结构-二叉树/","text":"1.概述二叉树是每个结点最多有两个子树的树结构。通常子树被称作“左子树(left subtree)”和“右子树(right subtree)”。二叉树常被用于二叉查找树和二叉堆。 2.分类 满二叉树：一棵深度为k，且有$2^k$-1个结点的二叉树。特点是每一层上的结点数都是最大结点树。 完全二叉树：除最后一层外，其余层都是满的，最后一层或者是满的，或者是右边缺少连续若干结点。 平衡二叉树：又被称为AVL树，是一棵二叉排序树，且具有以下性质：它是一棵空树，或它的左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是平衡二叉树。 3.定义及性质二叉树是递归定义的，逻辑上二叉树有五种基本形态。 (1)空二叉树——如图(a)；(2)只有一个根结点的二叉树——如图(b)；(3)只有左子树——如图(c)；(4)只有右子树——如图(d)；(5)完全二叉树——如图(e)。 4.遍历遍历是对树的一种最基本的运算，所谓遍历二叉树，就是按一定的规则和顺序走遍二叉树的所有结点，使每一个结点都被访问一次，而且只被访问一次。由于二叉树是非线性结构，因此，树的遍历实质上是将二叉树的各个结点转换成为一个线性序列来表示。 设L、D、R分别表示遍历左子树、访问根结点和遍历右子树， 则对一棵二叉树的遍历有三种情况： DLR（称为先根次序遍历） LDR（称为中根次序遍历） LRD （称为后根次序遍历） 层次遍历 即按照层次访问，通常用队列来做。访问根，访问子女，再访问子女的子女（越往后的层次越低） 5.与树的区别二叉树和树有许多相似之处，但二叉树不是树。 树中结点的最大度数没有限制，而二叉树结点的最大度数为2。 树的结点没有左右之分，而二叉树的结点有左右之分。","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"数据结构--B-树","date":"2019-03-03T11:30:14.000Z","path":"2019/03/03/数据结构-B-树/","text":"","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"数据结构--B+树","date":"2019-03-03T11:30:14.000Z","path":"2019/03/03/数据结构-B+树/","text":"","tags":[{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"JVM内存模型","date":"2019-02-27T12:30:14.000Z","path":"2019/02/27/JVM内存模型/","text":"也被合称为运行时数据区。 程序计数器（PC Register）最小的一块内存区域，记录了当前线程执行的字节码的行号，在虚拟机模型里，字节码解释器工作就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、异常处理、线程恢复等都要依赖程序计数器完成。 这是JVM规范中唯一一个没有规定会导致OOM的区域。 虚拟机栈（JVM Stack）每个方法被执行的时候都会创建一个“栈帧”，用于存储局部变量表（包括参数）、操作栈、方法出口等信息。每个方法被调用到执行完的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。生命周期与线程相同，是线程私有的。使用一段连续的内存空间。 栈帧（Stack Frame）是用于支持虚拟机进行方法调用和方法执行的数据结构，是虚拟机运行时数据区中的虚拟机栈的栈元素。 栈帧组成： 局部变量表（Local Variable Table）：是被组织为一个字长（32bit）为单位、从0开始计数的数组，用于存放方法参数和局部变量。存放了编译器可知的各种基本数据类型、对象引用（引用指针，非对象本身），其中64位长度的long和double类型的变量会占用2个局部变量的空间，其余数据类型只占1个。局部变量表所需的内存空间在编译期间完成分配（在方法的Code属性的max_locals数据项中确定），当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量是完全确定的，在运行期间栈帧不会改变局部变量表的大小空间。 操作数栈：也是被组织为一个字长（32bit）为单位的数组。但和局部变量表不一样的是它不是通过索引来访问，而是通过标准的栈操作（压栈和出栈）来访问的。可理解为Java虚拟机栈中的一个用于计算的临时数据存储区。 动态连接：指向运行时常量池中该栈帧所属方法的引用。持有这个引用是为了支持方法调用过程中的动态连接。Class 文件中存放了大量的符号引用，字节码中的方法调用指令就是以常量池中指向方法的符号引用作为参数。这些符号引用一部分会在类加载阶段或第一次使用时转化为直接引用，这种转化称为静态解析。另一部分将在每一次运行期间转化为直接引用，这部分称为动态连接。 方法返回：一个方法被执行后，有两种方式退出该方法。 1).执行引擎遇到任意一个方法返回的字节码指令，这时候可能会有返回值传递给上层的方法调用者。是否有返回值和返回值的类型将根据遇到何种方法返回指令来决定。该方式被称为正常完成出口（Normal Method Invocation Completion）。 2).方法执行过程中遇到了异常，并且这个异常没有在方法体内得到处理。无论是Java虚拟机内部产生的异常，还是代码中使用throw字节码指令产生的异常。只要在本方法的异常表中没有搜索到匹配的异常处理器，就会导致方法退出。这种方式被称为异常完成出口（Aburt Method Invocation Completion）。不会给它的调用者产生任何返回值。 附加信息：虚拟机规范允许具体的虚拟机实现增加一些规范里没有描述的信息到栈帧中，例如与高度相关的信息。这部分信息完全取决于具体的虚拟机实现。在实际开发中，一般会把动态连接、方法返回和附加信息归为一类，称为栈帧信息。 本地方法栈（Native Stack）类似虚拟机栈，区别在于虚拟机栈是为虚拟机执行的Java方法服务，而本地方法栈是为Native方法服务。 方法区（Method Area）也称为“永久代（Permanent Generation）”，用于存储虚拟机加载的类信息、常量（jdk7开始移到堆内存中）、静态变量，是线程共享的区域。 jdk8取消了方法区这个概念，取而代之的是元空间（Metaspace）。 运行时常量池（Runtime Constant Pool）方法区的一部分。用于存放编译器生成的各种字面量和符号引用。并非预置入Class文件中常量池的内容才进入常量池，运行期间页可能将新的常量放入池中，这种特性被开发人员利用的比较多的是String类的intern()方法。 堆（Heap）是Java虚拟机所管理内存中最大的一块内存区域，该区域存放了对象实例和数组（但不是所有的对象实例都在堆中，随着JIT编译器的发展，在编译期间，如果JIT经过逃逸分析，发现有些对象没有逃逸出方法，那么有可能堆内存分配会被优化成栈内存分配，也有可能还是在堆内存中分配。），生命周期与虚拟机相同。 在垃圾回收的时候，往往将堆内存分为新生代和老年代（大小比例为1：2），新生代又由Eden和Survivor0，Survivor1（也有称为from Survivor,to Survivor的）组成，三者的比例是8：1：1。新生代一般采用的是复制算法，老年代采用的是标记整理算法。 堆、方法区、运行时常量池为线程共享，程序计数器、本地方法栈、虚拟机栈为线程私有。 为什么要分开设计堆栈？ 栈存储了处理逻辑，堆存储了具体的数据，这样隔离设计更为清晰。 堆栈分离，使得堆可以被多个栈共享。 栈保存了上下文的信息，因此只能向上增长；而堆是动态分配。 直接内存 直接内存并不是虚拟机内存的一部分，也不是Java虚拟机规范中定义的内存区域。jdk1.4中新加入的NIO，引入了通道与缓冲区的IO方式，它可以调用Native方法直接分配堆外内存，这个堆外内存就是本机内存，不会影响到堆内存的大小。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"类加载工作机制","date":"2019-02-26T11:00:15.000Z","path":"2019/02/26/类加载工作机制/","text":"Java源码编译由三个过程组成：源码编译机制、类加载机制、类执行机制。 类的生命周期：加载(Loading)–验证(Verification)–准备(Preparation)–解析(Resolution)–初始化(Initialization)–使用(Using)–卸载(Unloading)。其中从加载到初始化五个阶段属于类加载的部分，验证–准备–解析三个阶段又合称为连接。 加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。 1系统可能在第一次使用某个类时加载该类，也可能采用预加载机制来加载某个类。 1.加载在加载阶段(可以参考java.lang.ClassLoader的loadClass()方法)，虚拟机主要完成三件事：1.1通过一个类的全限定名来获取定义此类的二进制字节流(并没有指明要从Class文件获取，也可以通过其他渠道，譬如：网络、动态生成、数据库等)1.2将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。1.3在内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 2.验证验证的主要目的是判断class文件的合法性，比如class文件一定是以0xCAFEBABE开头的，另外对版本号也会做验证，例如使用jdk1.8编译的class文件在jdk1.6虚拟机上运行，因为版本问题就会验证不通过。除此之外还会对元数据、字节码进行验证，具体的验证过程略过。 3.准备这个阶段正式为类变量（static修饰的变量）分配内存并设置初始值，这个内存分配是发生在方法区中。3.1这里并没有对实例变量进行内存分配，实例变量会在对象实例化时随着对象一起分配在Java堆中。3.2这里的初始值，是指数据类型的零值。 12private static int a = 3;这个类变量a是在准备阶段设置为0，在初始化阶段将3赋值给变量a。 但是如果是static final的变量，在准备阶段就会赋值。 12private static final int b = 1;这段代码在准备阶段就会将1赋值给变量b。 4.解析解析过程就是讲符号引用替换为直接引用。 注：符号引用就是一个Java源文件在被编译时，在不清楚被引用类实际内存地址的情况下，会使用唯一识别并定位到目标的符号来代替。如A类引用了B类，编译时A并不知道B类实际的内存地址，故可以使用能唯一识别B的符号来代替。而当类加载时，编译后的class文件实际已被调入内存，可知道A、B的实际内存地址，当引用的目标已被加载入内存，则此时的引用为直接引用。 5.初始化初始化是类加载的最后一步，这个时候才开始执行类中定义的Java程序代码。在这个阶段最重要的事情就是对类变量进行初始化，关注的重点是父子类之间各类资源初始化的顺序。Java中对类变量指定初始值有两种方式：1.声明类变量时指定初始值。2.使用静态初始化块为类变量指定初始值。初始化的时机3.1创建类实例的时候，分别有： 12345a.使用new关键字创建实例。b.通过反射创建实例。c.通过反序列化方式创建实例。 3.2调用某个类的类方法（静态方法）3.3访问某个类或接口的类变量，或为该类变量赋值。3.4初始化某个类的子类。当初始化子类的时候，该子类的所有父类都会被初始化。3.5直接使用java.exe命令来运行某个主类。 除了上面几种方式会自动初始化一个类，其他访问类的方式都称不会触发类的初始化，称为被动引用。如 12a.子类引用父类的静态变量，不会导致子类初始化。b.通过数组定义引用类，不会触发此类的初始化。 123456789101112131415161718public class SupClass&#123; public static int a = 123; static &#123; System.out.println(\"supclass init\"); &#125;&#125;public class Test&#123; public static void main(String[] args) &#123; SupClass[] spc = new SupClass[10]; &#125;&#125;// 控制台不会有输出。 1c.引用常量时，不会触发此类的初始化。因为用final修饰的类变量，在编译时就已经确定好放入常量池了，访问该类变量时，等于直接从常量池中获取，并没有初始化该类。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]},{"title":"类加载器","date":"2019-02-25T11:30:14.000Z","path":"2019/02/25/类加载器/","text":"1.概述什么是类加载器？ 类加载器就是负责读取Java字节代码，并转换成java.lang.Class类的一个实例。其本身也是一个类。 当一个Java项目启动的时候，JVM会找到main方法，根据对象之间的调用来对class文件和所引用的jar包中的class文件进行加载，方法区中开辟内存来存储类的运行时数据结构（包括静态变量、静态方法、常量池、类结构等），同时在堆中生成相应的Class对象指向方法区中对应的类运行时数据结构。 Class文件由类装载器装载后，在JVM中将形成一份描述Class结构的元信息对象，通过该元信息对象可以获知Class的结构信息：如构造函数、属性和方法等，Java允许用户借由这个Class相关的元信息对象间接调用Class对象的功能。 虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的Java类型，这就是虚拟机的类加载机制。 2.工作机制参考类加载工作机制。 3.类加载器（1）Bootstrap ClassLoader：引导类加载器，存放于&lt;JAVA_HOME&gt;\\lib目录中，或者被-Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中（仅按照文件名识别，如rt.jar，名字不符合的类库即使放在lib目录中也不会被加载）。用原生C++实现，无法被Java程序直接引用。 （2）Extension ClassLoader：扩展类加载器，存放于&lt;JAVA_HOME&gt;\\lib\\ext目录中，或者被java.ext.dirs系统变量所指定的路径中的所有类库加载。开发者可以直接使用扩展类加载器。 （3）Application ClassLoader：应用类加载器，负责加载用户类路径上（ClassPath）所指定的类库，开发者可直接使用。 （4）Custom ClassLoader：自定义类加载器，开发人员通过继承java.lang.ClassLoader类来实现自己的加载器，以满足一些特殊的要求。 4.双亲委派模型 工作过程：如果一个类加载器接收到了类加载的请求，它首先把这个请求委托给他的父类加载器去完成，每个层次的类加载器都是如此，因此所有的加载请求都应该传送到顶层引导类加载器中，只有当父加载器反馈自己无法完成这个加载请求时（也就是没有找到这个类），子加载器才会尝试自己去加载。 好处：Java类随着它的加载器一起具备了一种带有优先级的层次关系。例如类java.lang.Object，它存放在rt.jar中，无论哪个类加载器要加载这个类，最终都会委托给引导类加载器去加载，因此在程序的各种类加载器环境中都是同一个类。相反，如果用户自己写了一个名为java.lang.Object的类，并放在程序的ClassPath中，那系统中将会出现多个不同的Object类，java类型体系中最基础的行为也无法保证，应用程序会变得一片混乱。 顶层的ClassLoader无法加载底层ClassLoader的类。解决方法也有，JDK中提供了一个方法： 1Thread.setContextClassLoader() 用于解决顶层ClassLoader无法加载底层ClassLoader的类的问题，基本思想是在顶层的ClassLoader中，传入底层的ClassLoader的实例。 双亲委派模型的源码： 1234567891011121314151617181920212223242526272829303132333435363738394041protected Class&lt;?&gt; loadClass(String name, boolean resolve) throws ClassNotFoundException&#123; synchronized (getClassLoadingLock(name)) &#123; // 1. 检查是否已经加载过 Class c = findLoadedClass(name); if (c == null) &#123; long t0 = System.nanoTime(); try &#123; if (parent != null) &#123; //当前加载器存在父类加载器，递归调用父类加载器 c = parent.loadClass(name, false); &#125; else &#123; //检查该类是否被Bootstrap加载器加载过，有则返回，否则返回null c = findBootstrapClassOrNull(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // ClassNotFoundException thrown if class not found // from the non-null parent class loader &#125; if (c == null) &#123; // If still not found, then invoke findClass in order // to find the class. long t1 = System.nanoTime(); //既然父类加载器无法加载，则就是用用户自己所重载的类加载器 c = findClass(name); // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); &#125; &#125; if (resolve) &#123; //链接link所加载的类。该方法名起得不好，有误导性 resolveClass(c); &#125; return c; &#125;&#125; 5.类加载的三个机制委托、可见性、单一性。委托机制是指将加载一个类的请求交给父类加载器，如果父类加载器不能够找到或者加载这个类，那么再加载它。可见性的原理是子类的加载器可以看见所有的父类加载器已经加载的类，而父类加载看不到子类加载器加载的类。单一性原理是指仅加载一个类一次，这是由委托机制确保子类加载器不会再次加载父类加载器加载过的类。 6.Java虚拟机如何判定两个Class是相同的？JVM在判定两个class是否相同时，不仅要判断两个类名相同，还要判断是否由同一个类加载器实例加载的。只有两者同时满足的情况下，JVM才认为两个class是相同的。这里的相同包括类的equals()方法、isAssignableFrom()方法、isInstance()方法、instanceof关键字等判断出来的结果。 不同的类加载为相同名称的类创建了额外的名称空间。相同名称的类可以并存在Java虚拟机中，只需要用不同的类加载器来加载他们即可。不同类加载器加载的类之间是不兼容的，这就相当于在Java虚拟机内部创建了一个个相互隔离的Java类空间。这种技术在许多框架中都被用到，例如OGSI、Web容器（Tomcat），破坏了双亲模式。 OGSI的ClassLoader形成网状结构，根据需要自由加载Class。 Tomcat的WebappClassLoader就会先加载自己的Class，找不到再委托parent。 对于运行在 Java EE容器中的 Web 应用来说，类加载器的实现方式与一般的 Java 应用有所不同。不同的 Web 容器的实现方式也会有所不同。以 Apache Tomcat 来说，每个 Web 应用都有一个对应的类加载器实例。该类加载器也使用代理模式，所不同的是它是首先尝试去加载某个类，如果找不到再代理给父类加载器。这与一般类加载器的顺序是相反的。这是 Java Servlet 规范中的推荐做法，其目的是使得 Web 应用自己的类的优先级高于 Web 容器提供的类。这种代理模式的一个例外是：Java 核心库的类是不在查找范围之内的。这也是为了保证 Java 核心库的类型安全。 7.tips7.1可不可以自己写个String类？不可以，因为根据类加载的双亲委派机制，会去加载父类，父类发现冲突了String就不再加载了。（应该自己写的String类也要求是java.lang.String，这时候子类加载器会发现父类加载器已经加载过String类。） 7.2能否在加载类的时候，对类的字节码进行修改？可以，使用Java探针技术。","tags":[{"name":"JVM","slug":"JVM","permalink":"http://yoursite.com/tags/JVM/"},{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"}]}]